{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chrisk/env3/lib/python3.5/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['datetime']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from IPython.display import Audio\n",
    "import librosa\n",
    "import scipy as sp\n",
    "from numba import jit\n",
    "from scipy.spatial import distance\n",
    "\n",
    "figsize(20,6)\n",
    "prefix=\"baseline\"\n",
    "def filepre(nm):\n",
    "    return \"tmp/\"+prefix+\"_\"+nm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 10772298712735263252, name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 1256786126154002957\n",
       " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 16874224058943071659\n",
       " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10740894925\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "     link {\n",
       "       device_id: 1\n",
       "       type: \"StreamExecutor\"\n",
       "       strength: 1\n",
       "     }\n",
       "   }\n",
       " }\n",
       " incarnation: 10338563631680231163\n",
       " physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\", name: \"/device:GPU:1\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 10913290650\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "     link {\n",
       "       type: \"StreamExecutor\"\n",
       "       strength: 1\n",
       "     }\n",
       "   }\n",
       " }\n",
       " incarnation: 15819555088400326969\n",
       " physical_device_desc: \"device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:02:00.0, compute capability: 6.1\"]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "tfdevice='/device:GPU:0'\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sr = 22050\n",
    "def nrmse(output,target):\n",
    "    assert(output.shape[0] == target.shape[0])\n",
    "    combinedVar = 0.5 * (np.var(target, ddof=1) + np.var(output, ddof=1))\n",
    "    errorSignal = output - target\n",
    "    return np.sqrt(np.mean(errorSignal ** 2) / combinedVar)\n",
    "\n",
    "def generateInternalWeights(nInternalUnits, connectivity):\n",
    "    success = False\n",
    "    internalWeights = 0\n",
    "    while success == False:\n",
    "        try:\n",
    "            internalWeights = np.random.randn(nInternalUnits,nInternalUnits) * (np.random.random((nInternalUnits,nInternalUnits)) < connectivity)\n",
    "            specRad = max(abs(np.linalg.eig(internalWeights)[0]))\n",
    "            if (specRad > 0):\n",
    "                internalWeights = internalWeights / specRad\n",
    "                success = True\n",
    "        except e:\n",
    "            print(e)\n",
    "    return internalWeights\n",
    "\n",
    "pLoop = lambda n,p: p[n%p.size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "resultsFolderName = \"csynthResults/results\" + str(datetime.now()) + \"/\"\n",
    "os.mkdir(resultsFolderName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zhish.wav', 'kernelmix.wav', 'rocks.wav', 'boomwag.wav', 'snork.wav', 'insec3.wav', 'MacrosemiaTonk.wav', 'pattern10.wav', 'MeimunaNau.wav', 'backswing-old1.wav', 'audoubelclick2.wav', 'laboa.wav', 'swipe-old1.wav', 'Macrosemia.wav', 'revbell.wav', 'laekur.wav', 'birta.wav', 'clicks1.wav', 'clicko-old1.wav', 'bellx.wav', 'camina2.wav', 'dentist-old1.wav', 'heart.wav', 'electro.wav', 'bello.wav', 'camina1.wav', 'noinoi.wav', 'click.wav', 'InsectInfestationZG.wav', 'harshi-old1.wav', 'bellrip3.wav', 'iron.wav', 'patterndrone.wav', 'ausiclick.wav', 'knock.wav', 'rotatingIron.wav', 'firespark.wav', 'insec2.wav', 'camina4.wav', 'kicic.wav', 'phoo-old1.wav', 'bellrip2.wav', 'dalispark.wav', 'triplet.wav', 'glitch-old1.wav', 'ice.wav', 'Cryptotympana.wav', 'vindvabd.wav', 'pattern11.wav', 'camina3.wav', 'paper.wav', 'InsectFly.wav', 'skake2Ed.wav', 'kicicboom.wav', 'bellrip.wav', 'holeMONO.wav', 'spark.wav', 'elstatic-old1.wav', 'jump.wav', 'drr.wav', 'boom2.wav', 'flash-old1.wav', 'pattern8.wav', 'harshlow-old1.wav', 'zzzz.wav', 'crickBee.wav', 'convol2.wav', 'sweetmachine-old1.wav', 'kernel.wav', 'auclick.wav', 'InsecticideZG.wav', 'spade.wav', 'rain_ravi.wav', 'convol4.wav', 'InsectBee.wav', 'pork.wav', 'ironrip.wav', 'digaa.wav', 'wooo-old1.wav', 'dorje.wav', 'convol1.wav', 'auboom2.wav', 'bee.wav', 'clicko.wav', 'xylophone.wav', 'audoubleclick.wav', 'crackle3.wav', 'vindvaclick.wav', 'kernel2.wav', 'patterndrone3.wav', 'insectzapZG.wav', 'vindvahat.wav', 'auboom.wav', 'pattern9.wav', 'drone2.wav', 'magnetclock-old1.wav', 'convol5.wav', 'click-old1.wav', 'noise2.wav', 'blade-old1.wav', 'noise1.wav', 'kicic2.wav', 'CARinsect.wav', 'insec.wav', 'firespark2.wav', 'camclick-old1.wav', 'InsectBee2.wav', 'noise-old1.wav', 'auhiclick.wav', 'vindvadbase.wav', 'Dundunia.wav', 'convol3.wav', 'patterndrone2.wav', 'kerneldrone.wav', 'ravi.wav', 'machine.wav', 'boom.wav', 'chain.wav', 'firecrack.wav', 'woodsamp.wav', 'hapsi.wav', 'stretch.wav', 'drone1.wav', 'snow.wav', 'viromachine-old1.wav', 'chainSpade.wav', 'vindva.wav']\n"
     ]
    }
   ],
   "source": [
    "relevant_path = \"audios/ixi\"\n",
    "ixiFiles = [fn for fn in os.listdir(relevant_path)\n",
    "              if fn.endswith('wav')]\n",
    "# ixiFiles = ['909a_22k.wav']\n",
    "print(ixiFiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ixistr(id):\n",
    "    return str(id) + \"_\" + ixiFiles[id] + \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compareMFCCs(seq1, seq2):\n",
    "    fftSize=2048\n",
    "    hop=64    \n",
    "    melspec = librosa.feature.melspectrogram(y=seq1, sr=sr, n_fft=fftSize,hop_length=hop)\n",
    "    mfccs = librosa.feature.mfcc(S=melspec,n_mfcc=20)[1:,:]\n",
    "    melspec2 = librosa.feature.melspectrogram(y=seq2, sr=sr, n_fft=fftSize,hop_length=hop)\n",
    "    mfccs2 = librosa.feature.mfcc(S=melspec2,n_mfcc=20)[1:,:]    \n",
    "    return nrmse(mfccs.flatten(), mfccs2.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeConceptor(p, net, i_pattern, alpha):\n",
    "    print('Computing conceptor, alpha: ', alpha)\n",
    "#     Cs = np.zeros((4, 1), dtype=np.object)\n",
    "    R = net['patternRs'][0,i_pattern]\n",
    "    [U,s,V] = svd(R)\n",
    "#     s = svd(R, compute_uv=False)\n",
    "    S = tf.diag(s)\n",
    "#     Snew = (S.dot(linalg.inv(S + pow(alpha, -2) * np.eye(p['N']))))\n",
    "\n",
    "#     C =  U.dot(Snew).dot(U.T);\n",
    "    sinv = tf.matrix_inverse(tf.add(S, tf.multiply(double(pow(alpha, -2)), tf.eye(p['N'], dtype=float64))))\n",
    "    Snew = tf.matmul(S,sinv)\n",
    "#     Snew = tf.matmul(Snew, tf.eye(p['N'], dtype=float64))\n",
    "#     Snew = Snew.numpy()\n",
    "#     Snew = (S * linalg.inv(S + pow(alpha, -2) * np.eye(p['N'])))\n",
    "    tfU = tf.constant(U)\n",
    "    C = tf.matmul(tfU,Snew)\n",
    "    C = tf.matmul(C,tfU, adjoint_b=True)\n",
    "    return C\n",
    "\n",
    "\n",
    "def testConceptor(p, C, net, recallTestLength, tfW, tfWbias):\n",
    "    with tf.device(tfdevice):    \n",
    "        trials = 1\n",
    "        attens = np.zeros(trials)\n",
    "        LR = array(p['LR'])\n",
    "        LROneMinus = array(1.0 - p['LR'])\n",
    "        tfLR = tf.constant(LR)\n",
    "        tfLROneMinus = tf.constant(LROneMinus)\n",
    "#         tfWbias = tf.constant(net['Wbias'])\n",
    "#         tfW = tf.constant(net['W'])\n",
    "#         tfC = tf.constant(C)\n",
    "        for i_trial in range(trials):\n",
    "            x_CTestPL = np.zeros((p['N'], recallTestLength))\n",
    "            z_CTestPL = np.zeros((p['N'], recallTestLength))\n",
    "#             tfx_CTestPL = tf.TensorArray(tfW.dtype,p['N'])\n",
    "#             tfz_CTestPL = tf.TensorArray(tfW.dtype,p['N'])\n",
    "            x = tf.constant(0.5 * np.random.randn(p['N'],1))\n",
    "            for n in range(recallTestLength + p['washoutLength']):\n",
    "                xOld = tf.constant(x)\n",
    "                Wtarget = tf.matmul(tfW, x)\n",
    "                leakTerm = tf.multiply(LROneMinus,xOld)\n",
    "                newX =tf.tanh(tf.add(Wtarget, tfWbias))\n",
    "                newXLeaked = tf.multiply(LR,newX)\n",
    "                z = tf.add(leakTerm,newXLeaked)\n",
    "                x = tf.matmul(C,z)\n",
    "                if (n > p['washoutLength']):\n",
    "#                     tfx_CTestPL.write(n-p['washoutLength'], tf.transpose(x))\n",
    "#                     tfz_CTestPL.write(n-p['washoutLength'], tf.transpose(z))\n",
    "                    x_CTestPL[:,n-p['washoutLength']] = tf.transpose(x).numpy()\n",
    "                    z_CTestPL[:,n-p['washoutLength']] = tf.transpose(z).numpy()\n",
    "#             x_CTestPL = tfx_CTestPL.gather(tf.range(0,recallTestLength,1))\n",
    "#             z_CTestPL = tfz_CTestPL.gather(tf.range(0,recallTestLength,1))\n",
    "            attenuation = np.mean(pow(np.linalg.norm(z_CTestPL[:,:] - x_CTestPL[:,:], axis=1),2)) / np.mean(pow(np.linalg.norm(z_CTestPL[:,:], axis=1),2))\n",
    "            attens[i_trial] = attenuation\n",
    "\n",
    "    return np.mean(attens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeLoadedNetwork_v2(p):\n",
    "    Netconnectivity = 1\n",
    "    if p['N'] > 20:\n",
    "        Netconnectivity = 10.0/p['N'];\n",
    "    WstarRaw = generateInternalWeights(p['N'], Netconnectivity)\n",
    "    WinRaw = 2 * (np.random.rand(p['N'], 1) - 0.5)\n",
    "    WbiasRaw = 2 * (np.random.rand(p['N'], 1) - 0.5)\n",
    "\n",
    "    #Scale raw weights     \n",
    "    Wstar = p['NetSR'] * WstarRaw;\n",
    "    Win = p['NetinpScaling'] * WinRaw;\n",
    "    Wbias = p['BiasScaling'] * WbiasRaw;  \n",
    "    I = np.eye(p['N'])\n",
    "    x = np.zeros((p['N'],1))\n",
    "    \n",
    "    allTrainxArgs = np.zeros((p['N'] + 1, 0));\n",
    "    allTrainOldxArgs = np.zeros((p['N'], 0));\n",
    "    allTrainWtargets = np.zeros((p['N'], 0));\n",
    "    allTrainOuts = np.zeros((1, 0));\n",
    "    patternRs =  np.zeros((1, p['patts'].shape[0]), dtype=np.object)\n",
    "    print('Loading patterns: ', end='')\n",
    "    LR = array(p['LR'])\n",
    "    LROneMinus = array(1.0 - p['LR'])\n",
    "    for i_pattern in range(p['patts'].shape[0]):\n",
    "        print(i_pattern, \" \", end='')\n",
    "        patt = p['patts'][i_pattern]\n",
    "        pattLearnLen = patt.size * p['learnLength']\n",
    "        xCollector = np.zeros((p['N'] + 1, pattLearnLen));\n",
    "        xOldCollector = np.zeros((p['N'], pattLearnLen));\n",
    "        WTargetCollector = np.zeros((p['N'], pattLearnLen));\n",
    "        pCollector = np.zeros((1, pattLearnLen));\n",
    "        x = np.zeros((p['N'], 1))\n",
    "        tfWstar = tf.constant(Wstar)\n",
    "        tfWin = tf.constant(Win)\n",
    "        with tf.device(tfdevice):    \n",
    "\n",
    "            for n in range(p['washoutLength'] + pattLearnLen):\n",
    "                u = patt.take(n, mode='wrap')\n",
    "                xOld = x\n",
    "                Wtarget = (Wstar.dot(x)) + (Win.dot(u))\n",
    "#                 wstarx=tf.matmul(tfWstar,x)\n",
    "#                 winu = tf.multiply(tfWin,u)\n",
    "#                 Wtarget = tf.add(wstarx, winu)\n",
    "\n",
    "                leakTerm = LROneMinus.dot(xOld)\n",
    "                newX =tanh(Wtarget + Wbias)\n",
    "                newXLeaked = LR.dot(newX)\n",
    "                x = leakTerm + newXLeaked\n",
    "#                 xOldLR = tf.multiply(tf.constant(1.0-LR, dtype=float64), xOld)\n",
    "#                 biasedTarget = tf.add(Wtarget, Wbias)\n",
    "#                 biasedTarget = tf.tanh(biasedTarget)\n",
    "#                 biasedTargetLR = tf.multiply(tf.constant(LR, dtype=float64), biasedTarget)\n",
    "#                 x = tf.add(xOldLR, biasedTargetLR)\n",
    "\n",
    "                if n >= p['washoutLength']:\n",
    "                    xCollector[:, n - p['washoutLength']] = np.concatenate((x[:,0], np.array([1])))\n",
    "                    xOldCollector[:, n - p['washoutLength']] = xOld[:,0]\n",
    "                    WTargetCollector[:, n - p['washoutLength']] = Wtarget[:,0]\n",
    "                    pCollector[0, n - p['washoutLength']] = u\n",
    "\n",
    "                uOld = u\n",
    "\n",
    "            R = xCollector[0:-1].dot(xCollector[0:-1].T) / pattLearnLen\n",
    "            patternRs[0,i_pattern] = R\n",
    "            allTrainxArgs = np.concatenate((allTrainxArgs, xCollector), axis=1)\n",
    "            allTrainOldxArgs = np.concatenate((allTrainOldxArgs, xOldCollector), axis=1)\n",
    "            allTrainOuts = np.concatenate((allTrainOuts, pCollector), axis=1)\n",
    "            allTrainWtargets = np.concatenate((allTrainWtargets, WTargetCollector), axis=1)\n",
    "\n",
    "\n",
    "    Wout = (linalg.inv(allTrainxArgs.dot(allTrainxArgs.conj().T) +\n",
    "                      (p['TychonovAlphaReadout'] * np.eye(p['N'] + 1))).dot(allTrainxArgs).dot(allTrainOuts.conj().T)).conj().T\n",
    "\n",
    "    outsRecovered = Wout.dot(allTrainxArgs);\n",
    "    NRMSE_readout = mean(nrmse(outsRecovered, allTrainOuts))\n",
    "    absSize_readout = mean(mean(abs(Wout), axis=0))\n",
    "    print(\"\\nNRMSE readout: \", NRMSE_readout, \" :: \", end='')\n",
    "    print(\"absSize readout: \", absSize_readout)\n",
    "    \n",
    "    W = (linalg.inv(allTrainOldxArgs.dot(allTrainOldxArgs.conj().T) +\n",
    "                      (p['TychonovAlpha'] * np.eye(p['N']))).dot(allTrainOldxArgs).dot(allTrainWtargets.conj().T)).conj().T\n",
    "    NRMSE_W = mean(nrmse(W.dot(allTrainOldxArgs), allTrainWtargets))\n",
    "    absSize_W = mean(mean(abs(W), axis=0))\n",
    "    print(\"NRMSE W: \", NRMSE_W, \" :: \", end='')\n",
    "    print(\"absSize W: \", absSize_W)\n",
    "\n",
    "    data ={k: v for k, v in locals().items() if k in \n",
    "           ('p','Win','Wstar', 'Wbias','NRMSE_W', 'absSize_W','patternRs','W',\n",
    "            'Wout','NRMSE_readout', 'absSize_readout')}\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def render(p, patternCs, bestNet, lrMod=1.0, speed=1.0, xFade=0.05, srMod=1):\n",
    "    audio = np.zeros(0)\n",
    "    x = 0.5 * np.random.randn(p['N'],1)\n",
    "    C = patternCs[0]\n",
    "    \n",
    "    LR = p['LR']\n",
    "    LR = array(LR * lrMod)\n",
    "    LROneMinus = array(1.0 - p['LR'])\n",
    "    Wmod = bestNet['W'] * srMod\n",
    "    \n",
    "    #run to washout\n",
    "    for n in range(p['washoutLength']):\n",
    "        xOld = x\n",
    "        Wtarget = (Wmod.dot(x))\n",
    "        z = (LROneMinus.dot(xOld)) + (LR.dot(tanh(Wtarget + bestNet['Wbias'])))\n",
    "        x = C.dot(z)\n",
    "\n",
    "\n",
    "    for i_patt in range(p['patts'].shape[0]):\n",
    "        xFadeTime=int(p['patts'][i_patt].shape[0] * xFade)\n",
    "        for n in range(int(p['patts'][i_patt].shape[0] * speed)):\n",
    "            C = patternCs[i_patt]\n",
    "            v=int(p['patts'][i_patt].shape[0] * speed)\n",
    "            stepL = min(v - n - 1, xFadeTime)\n",
    "            stepU = min(n, xFadeTime)\n",
    "            m1 = 1.0\n",
    "            if(n > v-xFadeTime-1 and i_patt < p['patts'].shape[0]-1):\n",
    "                m1 = (stepL + stepU) / (2*xFadeTime)\n",
    "                nextC = patternCs[i_patt+1]\n",
    "                C = (m1 * C) + ((1.0-m1) * nextC)\n",
    "            else:\n",
    "                if (n < xFadeTime and i_patt > 0):\n",
    "                    m1 = 0.5 - (n / (2*xFadeTime))\n",
    "                    prevC = patternCs[i_patt-1]\n",
    "                    C = (m1 * prevC) + ((1.0-m1) * C)\n",
    "#                 else:\n",
    "#                     C = cNet['Cs'][0,0]\n",
    "            \n",
    "            xOld = x\n",
    "            Wtarget = (Wmod.dot(x))\n",
    "            z = (LROneMinus.dot(xOld)) + (LR.dot(tanh(Wtarget + bestNet['Wbias'])))\n",
    "            x = C.dot(z)\n",
    "\n",
    "            newSample = bestNet['Wout'].dot(np.concatenate((x[:,0], np.array([1]))))\n",
    "            audio = np.concatenate((audio, newSample))\n",
    "    return audio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalModel(genome, patterns, patternLengths, orgAudio, N=900):\n",
    "    LR =genome[0]\n",
    "    modelParams = {'N':N, 'NetSR':1.5, 'NetinpScaling':1.2,'BiasScaling':0.3, 'TychonovAlpha':0.0001,\n",
    "             'washoutLength':50, 'learnLength':4, 'TychonovAlphaReadout':0.0001,\n",
    "              'LR': LR,\n",
    "              'patts':patterns\n",
    "             }   \n",
    "    newNetwork  = makeLoadedNetwork_v2(modelParams)\n",
    "    with tf.device(tfdevice):\n",
    "        tfWbias = tf.constant(newNetwork['Wbias'])\n",
    "        tfW = tf.constant(newNetwork['W'])\n",
    "\n",
    "    \n",
    "    import scipy\n",
    "    def fitnessf(aperture, *args):\n",
    "        print('Pattern: ', args[0])\n",
    "        params = args[1]\n",
    "        net = args[2]\n",
    "        try:\n",
    "            C = computeConceptor(params, net, args[0], aperture)\n",
    "        except:\n",
    "            print(\"Exception when computing conceptor\")\n",
    "            return 999\n",
    "        atten = testConceptor(params, C, net, params['patts'][args[0]].size * params['learnLength'], args[3], args[4])\n",
    "        return atten\n",
    "\n",
    "    apertures = [scipy.optimize.fminbound(fitnessf, 0,1000,  disp=2, xtol=15, args = (x,modelParams, newNetwork, tfW, tfWbias)) \n",
    "                 for x in np.arange(modelParams['patts'].shape[0])]\n",
    "\n",
    "    #store conceptors with calculated apertures\n",
    "    patternCs = np.zeros(len(apertures), dtype=np.object)\n",
    "    for i_patt in range(patternCs.size):\n",
    "        patternCs[i_patt] = computeConceptor(modelParams, newNetwork, i_patt, apertures[i_patt]).numpy()\n",
    "        \n",
    "#     figsize(20,3)\n",
    "    audio = render(modelParams, patternCs, newNetwork, 1.0,1.0, 0.05)\n",
    "    error = compareMFCCs(audio, orgAudio)\n",
    "\n",
    "#     plot(audio)\n",
    "    return {\"error\":error, \"waveform\":audio, 'apertures':apertures, 'net':newNetwork}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalFitness(genome, data):\n",
    "\n",
    "    modelData = evalModel(genome, data['patterns'], data['patternLengths'], data['orgAudio'], data['N'])\n",
    "    if (\"winner\" in data):\n",
    "        if modelData['error'] < data['winner']['error']:\n",
    "            data['winner'] = modelData\n",
    "    else:\n",
    "        data['winner'] = modelData\n",
    "    return modelData['error']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "startTS = datetime.now()\n",
    "testing = False\n",
    "learningRates = linspace(0.05,0.95,10)\n",
    "# for currentIxi in range(len(ixiFiles)):\n",
    "for currentIxi in range(1) if testing else range(len(ixiFiles)):\n",
    "    def log(msg):\n",
    "        f = open(resultsFolderName + ixistr(currentIxi) + \"searchLog.txt\", \"a\")\n",
    "        f.write(str(datetime.now()) + \":\")\n",
    "        f.write(msg)\n",
    "        f.write('\\r\\n')\n",
    "        f.close()  \n",
    "        print(msg)\n",
    "    clear_output()\n",
    "    print(\"loading: \", ixiFiles[currentIxi])\n",
    "    y, sr = librosa.load(\"audios/\" + ixiFiles[currentIxi], sr=22050)\n",
    "    y = y[:5000] / np.max(y) * 0.5\n",
    "    print(sr)\n",
    "\n",
    "    #divide out windows\n",
    "    patterns = []\n",
    "\n",
    "    minPatternSize = 9\n",
    "    lastCrossing=0\n",
    "    for i in range(y.shape[0]-1):\n",
    "        if (i-lastCrossing) > minPatternSize and y[i] >=0 and y[i+1] < 0:\n",
    "            print(i)\n",
    "            segment = y[lastCrossing:i]\n",
    "            patterns.append(segment)\n",
    "            lastCrossing = i\n",
    "            \n",
    "    #convert to numpy\n",
    "    patterns = np.array(patterns, dtype=np.object)\n",
    "    maxPatterns = 150\n",
    "    patterns = patterns[:maxPatterns]\n",
    "    patternLengths = [x.shape[0] for x in patterns]\n",
    "    y = y[:sum(patternLengths)]\n",
    "    maxPatternLen = np.max(patternLengths)\n",
    "    minPatternLen = np.min(patternLengths)\n",
    "    print(\"max length: \", maxPatternLen)\n",
    "    print(\"min length: \", minPatternLen)\n",
    "    # for p in patterns:\n",
    "    #     plot(p)\n",
    "    print(patterns.shape)\n",
    "    \n",
    "    \n",
    "    data = {'patterns':patterns, 'patternLengths':patternLengths, 'orgAudio':y}\n",
    "    def onEpochStart():\n",
    "        clear_output()\n",
    "        print('Runtime:', print(datetime.now() - startTS))\n",
    "        print(currentIxi, '/', len(ixiFiles), ' : ', ixiFiles[currentIxi])\n",
    "        log(str(scores))\n",
    "\n",
    "#     brute force search of learning rates\n",
    "#     do the search at low res with smaller N\n",
    "    data['N'] = 600\n",
    "    scores = zeros_like(learningRates)\n",
    "    for i,l in enumerate(learningRates):\n",
    "        trials = 5\n",
    "        trialScores = zeros(trials)\n",
    "        for trial in range(trials):\n",
    "            onEpochStart()\n",
    "            log(\"lo res trial\" + str(trial))\n",
    "            log(str(scores))\n",
    "            log(str(trialScores))\n",
    "            trialScores[trial] = evalFitness(array([l]), data)\n",
    "        scores[i] = np.median(trialScores)\n",
    "        log(str(trialScores))\n",
    "    \n",
    "    winningScore = np.min(scores)\n",
    "    log(\"Winning score: \" + str(winningScore))\n",
    "    bestLR = learningRates[np.argmin(scores)]\n",
    "#     now generate the best of x at high res to find a good network\n",
    "    data['N'] = 900\n",
    "    trials = 10\n",
    "    scores = zeros(trials)\n",
    "    del data['winner']\n",
    "    for trial in range(trials):\n",
    "        onEpochStart()\n",
    "        log(\"hi res trial \" + str(trial))\n",
    "        scores[trial] = evalFitness(array([bestLR]), data)\n",
    "        log(str(scores))\n",
    "        \n",
    "        \n",
    "    \n",
    "    plt.close()\n",
    "    figsize(20,4)\n",
    "    plt.xlabel(\"Time (samples)\", fontsize=20)\n",
    "    plt.ylabel(\"Amplitude\", fontsize=20)\n",
    "    plt.xticks(fontsize=18, rotation=0)\n",
    "    plt.yticks(fontsize=18, rotation=0) \n",
    "    plot(y, label='Original', alpha=0.6)\n",
    "    plot(data['winner']['waveform'], alpha=1.0, label='Reconstruction')\n",
    "    plt.legend(fontsize=20)\n",
    "    plt.savefig(resultsFolderName + ixistr(currentIxi) + \"compare.pdf\", bbox_inches='tight')\n",
    "\n",
    "    librosa.output.write_wav(resultsFolderName + ixistr(currentIxi) + \"org.wav\",y, sr)\n",
    "    librosa.output.write_wav(resultsFolderName + ixistr(currentIxi) + \"recon.wav\", data['winner']['waveform'], sr)\n",
    "\n",
    "    import dill as pickle\n",
    "    with open(resultsFolderName + ixistr(currentIxi) + r\"model.dill.pickled\", \"wb\") as output_file:    \n",
    "        pickle.dump({'winner':data['winner'], 'original':y, 'patterns':patterns}, output_file, protocol=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
